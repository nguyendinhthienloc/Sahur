## Running the Pipeline (Terminal Instructions)

To run the main pipeline, statistical tests (1-vs-N), and IRAL lexical analysis on a dataset, use:

```
python -m src.cli run --input data/cleaned_by_topic/environment.csv --output results/environment_allcorpora_test --max-rows 10 --enable-embeddings --shards 2 --workers 2
```

- This will process the first 10 rows of environment.csv, extract features, run 1-vs-N statistical tests (Human vs all models), and run IRAL lexical analysis (frequencies, collocations, log-odds) for all corpora.
- Results will be saved in the specified output folder (e.g., `results/environment_allcorpora_test`).
- Figures, statistical tables, and IRAL outputs will be in subfolders.

**Requirements:** Activate your virtual environment and ensure all dependencies are installed before running.

```
.\.venv\Scripts\activate
```

Then run the pipeline command above.
# Human vs AI Text Classification: 6-Metric Linguistic Baseline

**Publication-ready linguistic feature extraction pipeline for distinguishing human and AI-generated text.**

This project provides a clean, maintainable implementation of six core linguistic metrics proven to differentiate human from AI writing, with optional IRAL-style lexical explainability features.

## âœ¨ Key Features

- **6 Core Metrics**: Focused, scientifically-grounded linguistic features
- **IRAL Integration**: Optional lexical explainability (Log-Odds, collocations)
- **Publication-Ready Figures**: IRAL journal-style visualizations
- **Robust Statistics**: Welch t-test, Mann-Whitney U, Cohen's d, FDR corrections
- **Efficient Processing**: Disk caching, parallel processing, resumable runs
- **Clean Architecture**: 10 modular files with single-responsibility design

---

## ğŸ¯ Six Core Metrics (Default Mode)

1. **MTLD** - Lexical diversity (Measure of Textual Lexical Diversity)
2. **Nominalization Density** - Derived nominalizations per 1000 words
3. **Modal/Epistemic Rate** - Modal verbs and epistemic markers per 100 words
4. **Clause Complexity** - Mean clausal dependencies per sentence
5. **Passive Voice Ratio** - Proportion of passive voice sentences
6. **S2S Cosine Similarity** - Mean sentence-to-sentence semantic similarity *(requires embeddings)*

**Why these 6?** Each metric captures a distinct linguistic dimension with proven discriminative power, low redundancy, and computational efficiency.


## ğŸ“¦ Installation

```bash
# Create virtual environment
python -m venv .venv

# Activate (Linux/Mac)
source .venv/bin/activate

# Activate (Windows)
.venv\Scripts\Activate.ps1

# Install dependencies
pip install -r requirements.txt

# Download spaCy model
python -m spacy download en_core_web_lg
```

### Core Dependencies

### ğŸŒ Cloud Environments (Colab/Kaggle)

For running on Google Colab or Kaggle notebooks:

```python
# Automated setup in one command
!git clone https://github.com/YOUR_USERNAME/AIvsHuman.git
%cd AIvsHuman
!python setup_environment.py
```

ğŸ“– **See [CLOUD_SETUP.md](CLOUD_SETUP.md) for complete cloud setup guide**


## âš¡ Quick Start & Complete Workflow

### Step 1: Extract Features

```bash
# Basic run: 6 core metrics without embeddings
python -m src.cli run \
  --input <PATH_TO_YOUR_INPUT_CSV> \
  --output <PATH_TO_OUTPUT_DIRECTORY> \
  --shards 4 \
  --workers 2
```

**Example:**
```bash
python -m src.cli run \
  --input data/gsingh1-train/train.csv \
  --output results/baseline_run \
  --shards 4 \
  --workers 2
```

**With embeddings (enables s2s_cosine_similarity):**
```bash
python -m src.cli run \
  --input data/gsingh1-train/train.csv \
  --output results/with_embeddings \
  --enable-embeddings \
  --shards 4 \
  --workers 2
```

**Outputs:**


### Step 2: Run Statistical Tests

âš ï¸ **Statistical tests are NOT automatic** - you must run them separately:

```bash
python -m src.cli analyze \
  --input <PATH_TO_OUTPUT_DIRECTORY>/all_features.parquet \
  --output <PATH_TO_OUTPUT_DIRECTORY>
```

**Example:**
```bash
python -m src.cli analyze \
  --input results/baseline_run/all_features.parquet \
  --output results/baseline_run
```

**Outputs:**


### Step 3: (Optional) Plots

Plots are no longer produced by the repository helper script. Use the CSV outputs in the `lexical/` folder to create figures locally with your preferred plotting tools.

Example: load `results/.../lexical/log_odds.csv` or `collocations_group_*.csv` into a notebook and generate visuals as needed.


### Step 4: IRAL Lexical Analysis (Optional)

For keyword extraction and log-odds analysis:

```bash
python -m src.cli analyze \
  --input <PATH_TO_OUTPUT_DIRECTORY>/all_features.parquet \
  --output <PATH_TO_OUTPUT_DIRECTORY> \
  --enable-iral-lexical
```

**Example:**
```bash
python -m src.cli analyze \
  --input results/baseline_run/all_features.parquet \
  --output results/baseline_run \
  --enable-iral-lexical
```

**Outputs:**


### Complete Example Workflow

```bash
# 1. Extract features with embeddings
python -m src.cli run \
  --input data/gsingh1-train/train.csv \
  --output results/full_analysis \
  --enable-embeddings \
  --shards 8 \
  --workers 2

# 2. Run statistical tests + IRAL lexical analysis
python -m src.cli analyze \
  --input results/full_analysis/all_features.parquet \
  --output results/full_analysis \
  --enable-iral-lexical

# 3. Generate plots (Manual)
# Use CSVs under `results/full_analysis/lexical/` to build figures locally.

# 4. View results
ls results/full_analysis/tables/
ls results/full_analysis/figures/
ls results/full_analysis/lexical/
```


### Quick Test Run

For testing with small sample:

```bash
python -m src.cli run \
  --input data/gsingh1-train/train.csv \
  --output results/test \
  --max-rows 100 \
  --shards 1
```


## ğŸ”§ Command-Line Options

### `python -m src.cli run` (Main Pipeline)

| Option | Description | Default |
|--------|-------------|---------|
| `--input, -i` | Input CSV file path | *required* |
| `--output, -o` | Output directory | *required* |
| `--text-col` | Text column name | auto-detect |
| `--label-col` | Label column name | auto-detect |
| `--topic-col` | Topic column name | auto-detect |
| `--max-rows` | Limit rows (for testing) | all |
| `--shards` | Number of processing shards | 1 |
| `--workers` | spaCy parallel workers | 1 |
| `--batch-size` | Parsing batch size | 32 |
| `--enable-embeddings` | Enable sentence embeddings | off |
| `--enable-iral-lexical` | Enable lexical analysis | off |
| `--cache-dir` | Cache directory | `{output}/cache` |
| `--dry-run` | Preview without executing | off |
| `--debug` | Debug logging | off |

### `python -m src.cli analyze` (Re-analyze Features)

Analyze pre-computed features without re-parsing:

```bash
python -m src.cli analyze \
  --input results/baseline_run/all_features.parquet \
  --output results/reanalysis \
  --enable-iral-lexical
```


## ğŸ“Š Output Structure

```
results/
â”œâ”€â”€ all_features.parquet      # Main output (compressed)
â”œâ”€â”€ all_features.csv           # Human-readable version
â”œâ”€â”€ metrics_schema.json        # Metric definitions
â”œâ”€â”€ cache/                     # Parse & embedding cache
â”‚   â””â”€â”€ shard_*/
â”œâ”€â”€ tables/
â”‚   â””â”€â”€ statistical_tests.csv # Welch, Mann-Whitney, Cohen's d, p-values
â”œâ”€â”€ figures/
â”‚   â”œâ”€â”€ violin_plots.png       # Metric distributions
â”‚   â”œâ”€â”€ radar_chart.png        # Metric profiles
â”‚   â””â”€â”€ keywords_*.png         # (if --enable-iral-lexical)
â””â”€â”€ lexical/                   # (if --enable-iral-lexical)
    â”œâ”€â”€ log_odds.csv
    â”œâ”€â”€ collocations_group_*.csv
    â””â”€â”€ top_freq_group_*.csv
```


## ğŸ§ª Testing

### Run Unit Tests

```bash
# All tests
pytest tests/test_core_metrics.py -v

# Specific test class
pytest tests/test_core_metrics.py::TestMTLD -v

# Fast mode (skip slow tests)
pytest tests/ -v -m "not slow"
```

### Integration Test

```bash
python -m src.cli run \
  --input data/gsingh1-train/train.csv \
  --output results/integration_test \
  --max-rows 50 \
  --shards 1

# Verify outputs
ls results/integration_test/all_features.parquet
ls results/integration_test/tables/statistical_tests.csv
ls results/integration_test/figures/
```


## ğŸ“š Documentation



## ğŸ—ï¸ Architecture

The pipeline consists of 10 modular Python files in `src/`:

1. **`cli.py`** - Command-line interface
2. **`ingest.py`** - Data loading and column detection
3. **`parse_and_cache.py`** - spaCy parsing with caching
4. **`metrics_core.py`** - Six core metric implementations
5. **`embeddings.py`** - Sentence embedding wrapper
6. **`stats_analysis.py`** - Statistical testing
7. **`iral_lexical.py`** - Optional lexical explainability
8. **`visualize.py`** - IRAL-style plotting
9. **`pipeline.py`** - Orchestration and shard processing
10. **`utils.py`** - Shared utilities

**Design principles:**


## ğŸ”¬ Statistical Analysis

The pipeline automatically computes:


All results exported to `tables/statistical_tests.csv`.


## ğŸ¨ Visualization

IRAL journal-style figures (serif fonts, grayscale, 300 DPI):


Figures saved to `{output}/figures/` in PNG format.


## âš™ï¸ Configuration

Edit `config/metrics_config.yaml` to customize:



## ğŸ“– Citation

If you use this pipeline in your research, please cite:

```bibtex
@software{6metric_baseline_2025,
  title={6-Metric Linguistic Baseline for Human vs AI Text Classification},
  author={Dinh Thien Loc Nguyen},
  year={2025},
  url={https://github.com/nguyendinhthienloc/AIvsHuman}
}
```

For IRAL lexical features, also cite the original IRAL paper.


## ğŸ› Troubleshooting

### "spaCy model not found"
```bash
python -m spacy download en_core_web_lg
```

### "sentence-transformers not installed"
```bash
pip install sentence-transformers
```

### "No text column found"
Specify column explicitly:
```bash
```

### Slow parsing
Increase batch size and workers:
```bash
```

### Out of memory
Reduce batch size or use more shards:
```bash
```


## ğŸ“ License

MIT License


## ğŸ¤ Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Add tests for new features
4. Submit a pull request


## ğŸ“ Contact

For questions or issues, please open a GitHub issue.


**Built with â¤ï¸ for linguistic analysis and human vs AI text classification research.**
